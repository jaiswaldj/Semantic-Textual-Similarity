{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Semantic Textual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "Given two paragraphs, quantify the degree of similarity between the two text-based on Semantic similarity. Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other. The STS task is motivated by the observation that accurately modelling the meaning similarity of sentences is a foundational language understanding problem relevant to numerous applications including machine translation (MT), summarization, generation, question-answering (QA), short answer grading, semantic search.  \n",
    "\n",
    "STS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs. \n",
    "\n",
    "\n",
    "The data contains a pair of paragraphs. These text paragraphs are randomly sampled from a raw dataset. Each pair of the sentence may or may not be semantically similar. The candidate is to predict a value between 0-1 indicating a degree of similarity between the pair of text paras. \n",
    "\n",
    "1 means highly similar  \n",
    "\n",
    "0 means highly dissimilar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # For Lemmetization of words\n",
    "from nltk.corpus import stopwords  # Load list of stopwords\n",
    "from nltk import word_tokenize # Convert paragraph in tokens\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from gensim.models import word2vec # For represent words in vectors\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Precily Assessment.zip', 'Precily_Assessment_DSFT.pdf', 'Text_Similarity_Dataset.csv', 'word2vec-GoogleNews-vectors-master', 'word2vec-GoogleNews-vectors-master.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"C:\\\\Users\\\\Deepak Jaiswal\\\\Desktop\\\\Data Science Project\\\\Precily Assignment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv(\"C:\\\\Users\\Deepak Jaiswal\\\\Desktop\\\\Data Science Project\\\\Precily Assignment\\\\Text_Similarity_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4023, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searchers fail to spot ads internet sear...   \n",
       "1          1  millions to miss out on the net by 2025  40% o...   \n",
       "2          2  young debut cut short by ginepri fifteen-year-...   \n",
       "3          3  diageo to buy us wine firm diageo  the world s...   \n",
       "4          4  be careful how you code a new european directi...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1  nasdaq planning $100m share sale the owner of ...  \n",
       "2  ruddock backs yapp s credentials wales coach m...  \n",
       "3  mci shares climb on takeover bid shares in us ...  \n",
       "4  media gadgets get moving pocket-sized devices ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the text dataset with rows and columns\n",
    "print(text_data.shape)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique_ID    0\n",
       "text1        0\n",
       "text2        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the text data if any null values\n",
    "text_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of text1 & text 2\n",
    "1. Convert phrases like won't to will not using function decontracted() below\n",
    "2. Remove Stopwords\n",
    "3. Remove any special symbol and lower case all words\n",
    "4. Lemmatizing words using WordNetLemmatizer define in function word_tokenizer below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\",\"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\",\"can not\", phrase)\n",
    "    \n",
    "    #general\n",
    "    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'re\",\" are\", phrase)\n",
    "    phrase = re.sub(r\"n\\'s\",\" is\", phrase)\n",
    "    phrase = re.sub(r\"n\\'d\",\" would\", phrase)\n",
    "    phrase = re.sub(r\"n\\'ll\",\" will\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'ve\",\" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'m\",\" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4023/4023 [08:43<00:00,  8.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "\n",
    "\n",
    "preprocessed_text1 = []\n",
    "\n",
    "# tqdm is for printing the status bar\n",
    "\n",
    "for sentance in tqdm(text_data['text1'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text1.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail spot ads internet search ...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions miss net 2025 40 uk population still ...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short ginepri fifteen year old...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo buy us wine firm diageo world biggest s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>careful code new european directive could put ...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  savvy searchers fail spot ads internet search ...   \n",
       "1          1  millions miss net 2025 40 uk population still ...   \n",
       "2          2  young debut cut short ginepri fifteen year old...   \n",
       "3          3  diageo buy us wine firm diageo world biggest s...   \n",
       "4          4  careful code new european directive could put ...   \n",
       "\n",
       "                                               text2  \n",
       "0  newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1  nasdaq planning $100m share sale the owner of ...  \n",
       "2  ruddock backs yapp s credentials wales coach m...  \n",
       "3  mci shares climb on takeover bid shares in us ...  \n",
       "4  media gadgets get moving pocket-sized devices ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging preprocessed _text1 in text_data\n",
    "\n",
    "text_data['text1'] = preprocessed_text1\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "          #tokenizes and stems the next\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings:\n",
    "\n",
    "1. Word embeddings are low dimensional vectors obtained by training a neural network on a large corpus to predict a word given  context (Continuous Bag Of Words model) or to predict the context given a word (skip gram model). The context is a window of surrounding words. Pre-trained word embeddings are also available in the word2vec code.google page.\n",
    "\n",
    "2.In this i am using Google news pre trained vectors and compare similarity between text1 & text2 using n_similarity method in gensim library which is nothing compares cosine similarity between two\n",
    "\n",
    "3.Consider it as a unsupervised problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre_trained Google News Vectors after download file\n",
    "\n",
    "wordmodelfile=\"GoogleNews-vectors-negative300.bin.gz\"\n",
    "wordmodel= gensim.models.KeyedVectors.load_word2vec_format(wordmodelfile, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code check if word in text1 & text2 present in our google news vectors vocabalry.\n",
    "# if not it removes that word and if present it compares similarity score between text1 and text2 words\n",
    "\n",
    "\n",
    "similarity = [] # List for store similarity score\n",
    "\n",
    "\n",
    "\n",
    "for ind in text_data.index:\n",
    "    \n",
    "        s1 = text_data['text1'][ind]\n",
    "        s2 = text_data['text2'][ind]\n",
    "        \n",
    "        if s1==s2:\n",
    "                 similarity.append(0.0) # 0 means highly similar\n",
    "                \n",
    "        else:   \n",
    "\n",
    "            s1words = word_tokenizer(s1)\n",
    "            s2words = word_tokenizer(s2)\n",
    "            \n",
    "           \n",
    "            \n",
    "            vocab = wordmodel.vocab #the vocabulary considered in the word embeddings\n",
    "            \n",
    "            if len(s1words and s2words)==0:\n",
    "                    similarity.append(1.0)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                for word in s1words.copy(): #remove sentence words not found in the vocab\n",
    "                    if (word not in vocab):\n",
    "                           \n",
    "                            \n",
    "                            s1words.remove(word)\n",
    "                        \n",
    "                    \n",
    "                for word in s2words.copy(): #idem\n",
    "\n",
    "                    if (word not in vocab):\n",
    "                           \n",
    "                            s2words.remove(word)\n",
    "                            \n",
    "                            \n",
    "                similarity.append((1-wordmodel.n_similarity(s1words, s2words))) \n",
    "                # as it is given 1 means highly dissimilar & 0 means highly similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.301230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.238022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.282121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  Similarity_score\n",
       "0          0          0.301230\n",
       "1          1          0.238022\n",
       "2          2          0.282121"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Unique_ ID and similarity\n",
    "\n",
    "final_score = pd.DataFrame({'Unique_ID':text_data.Unique_ID,\n",
    "                            'Similarity_score':similarity})\n",
    "final_score.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF as CSV file\n",
    "final_score.to_csv('final_score.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd Method (Jaccard similarity)\n",
    "\n",
    "Jaccard similarity or intersection over union is defined as size of intersection divided by size of union of two sets. Let’s take example of two sentences:\n",
    "\n",
    "* Sentence 1: AI is our friend and it has been friendly\n",
    "* Sentence 2: AI and humans have always been friendly\n",
    "\n",
    "In order to calculate similarity using Jaccard similarity, we will first perform lemmatization to reduce words to the same root word. In our case, “friend” and “friendly” will both become “friend”, “has” and “have” will both become “has”.\n",
    "For the above two sentences, we get Jaccard similarity of 5/(5+3+2) = 0.5 which is size of intersection of the set divided by total size of set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity check with Jaccard Similarity\n",
    "def get_jaccard_sim(str1, str2):\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float (len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = [] # List for store similarity score\n",
    "\n",
    "\n",
    "\n",
    "for ind in text_data.index:\n",
    "    \n",
    "        s1 = text_data['text1'][ind]\n",
    "        s2 = text_data['text2'][ind]\n",
    "        similarity.append(1-get_jaccard_sim(s1, s2))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.970356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.978528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.975207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  Similarity_score\n",
       "0          0          0.970356\n",
       "1          1          0.978528\n",
       "2          2          0.975207"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Unique_ ID and similarity\n",
    "\n",
    "final_score = pd.DataFrame({'Unique_ID':text_data.Unique_ID,\n",
    "                            'Similarity_score':similarity})\n",
    "final_score.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF as CSV file\n",
    "final_score.to_csv('Jaccard_Similarity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
